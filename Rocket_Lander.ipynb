{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env = env.unwrapped\n",
    "state  = env.reset()\n",
    "action = env.action_space\n",
    "\n",
    "\n",
    "# Policy gradient has high variance, seed for reproducability\n",
    "env.seed(0)\n",
    "np.random.seed(seed=0)\n",
    "#####################\n",
    "\n",
    "scores = []\n",
    "print(\"env.action_space\", env.action_space)\n",
    "print(\"env.observation_space\", env.observation_space)\n",
    "\n",
    "episodes = 1000\n",
    "steps    = 100\n",
    "episode_actions = []\n",
    "episode_states  = []\n",
    "for _ in range(episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    state = state.reshape(1,-1)  \n",
    "    score = 0.0\n",
    "    \n",
    "    for _ in range(steps): \n",
    "        action_step = np.random.normal(loc=0.0, scale=0.7, size=2)\n",
    "        action_step = np.clip(action_step, -1.0, 1.0)\n",
    "        state, reward, done, _ = env.step(action_step)\n",
    "\n",
    "        score += reward\n",
    "        #Store information for debugging and plotting\n",
    "        episode_actions.extend([action_step])\n",
    "        episode_states.append([state])\n",
    "        \n",
    "        # Stopping conditions\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "    #Store cumulative reward for this episode        \n",
    "    scores.append(score)   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "smooth = 5\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(scores, '.', alpha=0.25, color='xkcd:blue')\n",
    "plt.plot(np.convolve(scores, np.ones(smooth)/smooth)[(smooth-1)//2:-smooth], \n",
    "         color='xkcd:blue', \n",
    "         label='Total Reward')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.legend(loc=2)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.xlim(0, len(scores))\n",
    "plt.ylim(np.mean(scores)-5*np.std(scores), np.mean(scores)+5*np.std(scores))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "mu    = np.mean(scores)\n",
    "sigma = np.std(scores)\n",
    "\n",
    "num_bins = 40\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.style.use('seaborn-white')\n",
    "n, bins, patches = plt.hist(scores, num_bins,  alpha=0.9, color='steelblue', edgecolor='black')\n",
    "\n",
    "# add a 'best fit' line\n",
    "#y = norm.pdf(bins, mu, sigma)\n",
    "#plt.plot(bins, y, 'r--')\n",
    "plt.xlabel('Rewards')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(r'Rewards Histogram: $\\mu={:.2f}$, $\\sigma={:.2f}$'.format( mu, sigma))\n",
    " \n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "#plt.subplots_adjust(left=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "x1 = np.linspace(1, 8, 8, endpoint=True)\n",
    "states_mean = np.mean(np.asarray(episode_states),axis=0)\n",
    "plt.bar(x1, states_mean[-1], alpha=0.25, color='xkcd:blue')\n",
    "plt.xlabel('States')\n",
    "plt.ylabel('Mean')\n",
    "plt.title(r'Mean of State Space Components')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "x1 = np.linspace(1, 8, 8, endpoint=True)\n",
    "states_sigma = np.std(np.asarray(episode_states),axis=0)\n",
    "plt.bar(x1, states_sigma[-1], alpha=0.25, color='xkcd:blue')\n",
    "plt.xlabel('States')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.title(r'Standard Deviation of State Space Components')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = np.divide((np.asarray(episode_states) - states_mean),states_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "x1 = np.linspace(1, 8, 8, endpoint=True)\n",
    "normalized_mean = np.mean(normalized,axis=0)\n",
    "plt.bar(x1, normalized_mean[-1], alpha=0.25, color='xkcd:blue')\n",
    "plt.xlabel('States')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.title(r'Standard Deviation of State Space Components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Deterministic Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load Main Modules\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "import time\n",
    "#from sklearn.preprocessing import normalize\n",
    "\n",
    "#Load Graphical Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "# Load Custom Modules\n",
    "from Task.SoftLanding import SoftLanding\n",
    "from Agents.agent import DDPG\n",
    "\n",
    "# Check GPU compatibility \n",
    "#K.tensorflow_backend._get_available_gpus()\n",
    "#config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 70} ) \n",
    "#sess = tf.Session(config=config) \n",
    "#K.tensorflow_backend.set_session(sess)\n",
    "\n",
    "########################################################\n",
    "\n",
    "num_episodes =1400 # number of episodes\n",
    "\n",
    "#Create agent\n",
    "Q_targets_next = ([])\n",
    "Q_targets      = ([])\n",
    "\n",
    "random_seed = 0\n",
    "\n",
    "def train(num_episodes=300, max_t=20000, print_every=5):\n",
    "    \n",
    "    # Initialize training\n",
    "    task = SoftLanding()\n",
    "    agent = DDPG(task, random_seed)\n",
    "\n",
    "    #####################\n",
    "    RENDER_ENV = False\n",
    "    score_average  = []\n",
    "    score_average2 = []\n",
    "    scores = []\n",
    "    best_score = -np.inf\n",
    "    best_episode_states = []\n",
    "    best_episode_actions = []\n",
    "\n",
    "    \n",
    "    for episode in range(num_episodes + 1):\n",
    "\n",
    "        # Initialize episode\n",
    "        state = agent.reset_episode()\n",
    "        done = False\n",
    "        state = state.reshape(1,-1)  \n",
    "        score = 0.0\n",
    "        steps = 0\n",
    "        tic = time.time()\n",
    "        \n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        \n",
    "        # Execute\n",
    "        for t in range(max_t):\n",
    "            if (RENDER_ENV): task.env.render()\n",
    "            #state = np.divide((np.asarray(state) - states_mean),states_sigma)    \n",
    "            action = agent.act(state)\n",
    "            #print(action)\n",
    "             \n",
    "            # Turn off engines if lander touch the ground   \n",
    "            if ((state[-1,6] != 1.0) and (state[-1,7] != 1.0)) :\n",
    "                # Evaluate possible actions\n",
    "                action = agent.act(state)\n",
    "                #print(action)\n",
    "                action = np.clip(action, task.action_low, task.action_high)\n",
    "            else:\n",
    "                #Turn off the engine whe\n",
    "                action = np.zeros(agent.action_size)\n",
    "                \n",
    "            # Apply actions to the environment\n",
    "            next_state, reward, done = task.step(action)\n",
    "            next_state = next_state.reshape(1,-1) \n",
    "            #next_state = np.divide((np.asarray(next_state) - states_mean),states_sigma)  \n",
    "            \n",
    "            # Learn action/state pair\n",
    "            agent.step(action, reward, next_state, done, t)\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            # Calculate Total reward of the episode\n",
    "            score += reward\n",
    "            steps += 1\n",
    "            \n",
    "            #Store information for debugging and plotting\n",
    "            episode_actions.append(action)\n",
    "            episode_states.append(state)\n",
    "         \n",
    "            Q_targets_next.extend([agent.Q_targets_next])\n",
    "            Q_targets.append(agent.Q_targets)           \n",
    "            \n",
    "            # Stopping conditions\n",
    "            if done:\n",
    "\n",
    "                break \n",
    "                \n",
    "            if score < -300:\n",
    "                break\n",
    "                \n",
    "            # Lander angle is outside the control limits\n",
    "            if (state[-1,4] < -0.7 or state[-1,4] > 0.7):\n",
    "                break    \n",
    "\n",
    "            toc = time.time()\n",
    "            elapsed_sec = toc - tic\n",
    "            \n",
    "            #Timeout condition\n",
    "            if elapsed_sec > 60:\n",
    "                done = True  \n",
    "                \n",
    "        #Store cumulative reward for this episode        \n",
    "        scores.append(score)   \n",
    "        avg  = sum(scores)/max(1,len(scores))\n",
    "        avg2 = score/max(1,steps)\n",
    "        score_average.append(avg)\n",
    "        score_average2.append(avg2)\n",
    "\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            episode_states = episode_states\n",
    "            episode_actions = episode_actions\n",
    "            \n",
    "        if score >= 200.0:            \n",
    "            print('\\nEnvironment solved in {:d} episodes!\\t Score: {:.2f}'.format(episode, score)) \n",
    "            print('\\rEpisode {}, Score: {:.2f}, Best: {:.2f}, Min: {:.2f}, Time: {:.2f}'\\\n",
    "                      .format(episode, score, best_score, np.min(scores), elapsed_sec), end=\"\\n\")\n",
    "            print(action)\n",
    "            print(next_state)\n",
    "            #print(episode_states)\n",
    "            #print(episode_actions)\n",
    "            #print(reward)\n",
    "            print(agent.epsilon)\n",
    "            agent.epsilon  = agent.epsilon * 0.8\n",
    "            #break    \n",
    "            \n",
    "            \n",
    "        if episode % print_every == 0:\n",
    "            print('\\rEpisode {}, Score: {:.2f}, Best: {:.2f}, Min: {:.2f}, Time: {:.2f}'\\\n",
    "                      .format(episode, score, best_score, np.min(scores), elapsed_sec), end=\"\\n\")\n",
    "            print(action)\n",
    "            print(next_state)\n",
    "            #print(reward)\n",
    "            print(agent.epsilon)\n",
    "            \n",
    "        \n",
    "    \n",
    "    return scores, score_average, score_average2\n",
    "\n",
    "\n",
    "scores, score_average, score_average2 = train(num_episodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(score_average)+1), score_average)\n",
    "plt.ylabel('Average Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 5\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(scores, '.', alpha=0.25, color='xkcd:blue')\n",
    "plt.plot(np.convolve(scores, np.ones(smooth)/smooth)[(smooth-1)//2:-smooth], \n",
    "         color='xkcd:blue', \n",
    "         label='Total Reward')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.legend(loc=2)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.xlim(0, len(scores))\n",
    "plt.ylim(np.mean(scores)-5*np.std(scores), np.mean(scores)+5*np.std(scores))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(score_average2)+1), score_average2)\n",
    "plt.ylabel('Average Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "from scipy.stats import norm\n",
    "#%matplotlib inline\n",
    "\n",
    "mu    = np.mean(scores)\n",
    "sigma = np.std(scores)\n",
    "\n",
    "num_bins = 40\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.style.use('seaborn-white')\n",
    "n, bins, patches = plt.hist(scores, num_bins,  alpha=0.9, color='steelblue', edgecolor='black')\n",
    "\n",
    "# add a 'best fit' line\n",
    "#y = norm.pdf(bins, mu, sigma)\n",
    "#plt.plot(bins, y, 'r--')\n",
    "plt.xlabel('Rewards')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(r'Rewards Histogram: $\\mu={:.2f}$, $\\sigma={:.2f}$'.format( mu, sigma))\n",
    " \n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "#plt.subplots_adjust(left=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_targets_next_flat = []\n",
    "for sublist in Q_targets_next[128:]:\n",
    "    for val in sublist:\n",
    "        Q_targets_next_flat.append(val)\n",
    "Q_targets_flat = []\n",
    "for sublist in Q_targets[128:]:\n",
    "    for val in sublist:\n",
    "        Q_targets_flat.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "bx = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(Q_targets_flat, Q_targets_next_flat)\n",
    "plt.ylabel('Q Target Prediction')\n",
    "plt.xlabel('Q Target Calculated')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "bx = fig.add_subplot(111)\n",
    "plt.plot(Q_targets_next_flat, 'r', alpha=0.4) \n",
    "plt.plot(Q_targets_flat,'g',alpha=0.3)\n",
    "\n",
    "plt.ylabel('Q Value')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
